name: Update Data and Deploy to GitHub Pages

on:
    schedule:
        - cron: "*/15 * * * *" # Runs every 45 minutes

permissions:
    contents: write
    pages: write

jobs:
    update_and_deploy:
        runs-on: ubuntu-latest

        steps:
            # Step 1: Checkout the repository
            - name: Checkout code
              uses: actions/checkout@v3

            # Step 2: Set up Python
            - name: Set up Python
              uses: actions/setup-python@v4
              with:
                  python-version: "3.x"

            # Step 3: Install dependencies
            - name: Install dependencies
              run: |
                  python -m pip install --upgrade pip
                  pip install -r fb-scrapper/requirements.txt

            # Step 4: Set environment variables
            - name: Set environment variables
              run: |
                  echo "API_TOKEN=${{ env.API_TOKEN }}" >> $GITHUB_ENV
                  echo "NEXT_PUBLIC_PAGE_ID=${{ env.NEXT_PUBLIC_PAGE_ID }}" >> $GITHUB_ENV
                  echo "NEXT_PUBLIC_DATA_URL=${{ env.NEXT_PUBLIC_DATA_URL }}" >> $GITHUB_ENV
              env:
                  API_TOKEN: ${{ secrets.API_TOKEN }}
                  NEXT_PUBLIC_PAGE_ID: ${{ secrets.NEXT_PUBLIC_PAGE_ID }}
                  NEXT_PUBLIC_DATA_URL: ${{ secrets.NEXT_PUBLIC_DATA_URL }}

            # Step 5: Run the Python scraper script
            - name: Run scraper
              run: python fb-scrapper/scrapper.py

            # Optional: Commit and push the updated data.json to GitHub
            - name: Commit and push updated data.json
              run: |
                  git config --global user.name 'SamratAdhikari'
                  git config --global user.email 'samratmetaladhikari@gmail.com'
                  git add public/data.json
                  git commit -m 'Update data.json with new posts data'
                  git push
              env:
                  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
